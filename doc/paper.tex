\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Large Language Models in Reinforcement Learning - A Comparison of Methods}

\author{\IEEEauthorblockN{Adrian Duric}
\IEEEauthorblockA{\textit{Master Student, Dept. Informatics} \\
\textit{The faculty of Mathematics}\\
Oslo, Norway \\
adriandu@ifi.uio.no}
\and
\IEEEauthorblockN{Gregor Kajda}
\IEEEauthorblockA{\textit{Master Student, Dept. of Informatics} \\
\textit{The faculty of Mathematics}\\
Oslo, Norway \\
grzegork@ifi.uio.no}
\and
\IEEEauthorblockN{Jonatan Hoffmann Hanssen}
\IEEEauthorblockA{\textit{Master Student, Dept. of Informatics} \\
\textit{The faculty of Mathematics}\\
Oslo, Norway \\
jonatahh@ifi.uio.no}
}

\maketitle

\begin{abstract}

Reinforcement Learning (RL) algorithms suffer when rewards are sparce and the state-action space is large. Even tasks which appear relatively simple can prove intractable if the completion of the task requires subtasks to be completed first, or if the reward only comes when the entire task has been completed. In such cases, random exploration is unlikely to lead the agent to discover a solution to the problem. The apparent simplicity of such problems is often due to human intuition, which allow us to quickly see possible solutions to a diverse set of problems. Large Language Models (LLMs) are trained on large corpura of human written text, and have been shown to capture parts of this intuition in many tasks \cite{sparks}. In recent years, LLMs have been succesfully used to aid RL agents in more efficient exploration, and have allowed them to solve problems which previous methods have been unable to \cite{omni} \cite{ellm}. We compare different methods for integrating an LLM into the deep learning reinforcement algorithm proximal policy optimization (PPO), and compare their efficiency against eachother. We find that BEST METHOD gives the best sample efficiency, outperforming normal PPO by PERCENTAGE in METRIC.
\end{abstract}

\begin{IEEEkeywords}
large language models, reinforcement learning, minigrid, proximal policy optimization
\end{IEEEkeywords}

\section{Introduction}

One of the central challenges of reinforcement learning is that rewards are often both extremely rare and delayed, which means that optimizing a reinforcement learning algorithm requires significant trial and error \cite[423]{brunton}. Furthermore, in many problems the state-action spaces are enormous, meaning that uniform exploration is unlikely to find good solutions in a reasonable amount of time. Many methods have been developed to deal with this issue, for example by introducing auxilliary reward functions that use domain knowledge to reward actions which are considered good, or which reward the agent for learning novel skills. However, hand picking which actions to reward can lead to imitation rather than optimal behaviour, and novelty is not always useful \cite[1]{ellm}. In recent years, LLMs have shown remarkable capabilities in problem solving and planning \cite{sparks}, qualities which traditional RL agents often lack. Thus, a new area of research has emerged, which attemps to use the vast amount of human knowledge encoded in these models to increase the performance of reinforcement learning algorithms \cite{survey}. These LLM assisted RL agents have been able to outperform state-of-the-art RL methods in many problems \cite{omni} \cite{ellm} \cite{idm}.
\\

These papers explore different methods of integrating LLMs into a standard reinforcement learning training loop, from altering the policy directly to introducing an auxilliary reward function. In this paper, we compare both methods in the same environment using the same deep reinforcement algorithm (PPO), and compare their results to eachother to explore how LLMs best can be integrated into RL.




\section{Background and Related Work}

 Degus is the future in blockchain techonology, created by using the revolutionary "degus system", which creates twice as many coins per unit of computational power. By accepting the "degus mindset", you will be able to acheive things you never ever thought possible. The degus mindset includes the following:

Always Striving for Excellence
Generate passive revenue through the degus system
Live day by day: "Carpe diem"

By following these simple steps, and by buying several GPUs, you will be able to produce around 15 deguscoins per computational unit, unleashing the true power of the blockchain.

What is the deguscoin, you may ask? The deguscoin represents a complete transformation of the traditional financial system (tradfi). By using deguscoin, you will be able to achieve returns of investment (ROIs) beyond mere human comprehension. One single deguscoin, when inserted into an economy, can have such destructive power that even mere days after, the market will be beyond recognition. By invoking the "degus mindset" (DM) you will be able to send around 15\% more currency per coin, achieving ROIs which would send the average economist into a seizure.

Market players are predicting that by the end of this year, around 50\% of all transactions will be done under the degus system (DS), not even counting derivatives.

By the end of this year, around 50\% of all transactions will be done under the degus system, not even counting derivatives.

– Market players

What are you waiting for? Purchase deguscoin right now, and leave your earthly desires behind! 

\section{Method}

Deguser trenger god plass for å trives, og du bør investere i et størst mulig bur som gir dyrene mulighet til å klatre, grave i et tykt strølag og bevege seg i flere etasjer.

Mange dyreeiere konstruerer sine egne løsninger, for eksempel med et stort, gammelt akvarium som underdel og et nettingbur på toppen av dette. Da får du full oversikt over hva som skjer også nederst i buret, samtidig som du slipper at sand, strø og høy søles ut i rommet. Ikke la degusene gå direkte på gitterbunn eller annet hardt underlag, det kan skade føttene.

Buret kan du innrede med f.eks. røtter og greiner til å klatre på, hus av treverk, rør til å krype gjennom og løpehjul. Et løpehjul til degus bør være minst 25 cm i diameter og ha tett gulv og vegger slik at halen ikke kan komme i klem. Bunnmaterialet i buret bør være et tykt lag av støvfritt smådyrstrø som er egnet for graving. Det finnes flere egnede strøtyper å få kjøpt. Bland gjerne inn litt tørr blomsterjord eller sand i bunnmaterialet. Hvis du henter sand ute, bør du desinfisere den ved frysing eller steking fr den brukes i buret. Degusene trenger også tilgang på fiberrikt materiale som høy, revet papir eller treull til bygge- og redemateriale.

For å være sikker på at alle degusene som bor sammen kan trekke seg unna og hvile, bør de få hvert sitt sovehus. Deguser kan også finne på å forsvare maten sin overfor andre deguser. Når flere deguser bor i samme bur, bør du derfor ha flere matskåler plassert på forskjellige steder i buret slik at alle får tilgang til mat når de ønsker det. Dyrene må alltid ha tilgang på friskt vann, som best gis i en drikkeflaske.

For å holde pelsen i orden har deguser behov for å bade i finkornet sand flere ganger i uka. Slik badesand får du kjøpt i dyrebutikken. Et tungt kar som ikke velter så lett er fint som sandbadekar. Det kan være lurt å fjerne sandbadet mellom hvert bad for å unngå at den brukes som toalett.

Rengjøring

Hvor ofte buret bør rengjøres avhenger av størrelsen og hvor mange som bor der. Som en tommelfingerregel bør du skifte bunnmaterialet og vaske bunn og innredning én gang i uka. 

\section{Experiments}

\section{Results}

\section{Conclusion}


\section*{Acknowledgment}

I would like to acknowledge myself for being a absolute legend.

\section*{References}


\bibliographystyle{apalike}
\bibliography{bibliography}

\end{document}
