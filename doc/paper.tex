\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Large Language Models in Reinforcement Learning - A Comparison of Methods}
\thispagestyle{plain}
\pagestyle{plain}

\author{\IEEEauthorblockN{Adrian Duric}
\IEEEauthorblockA{\textit{Master Student, Dept. of Informatics} \\
\textit{The Faculty of Mathematics}\\
\textit{and Natural Sciences}\\
Oslo, Norway \\
adriandu@ifi.uio.no}
\and
\IEEEauthorblockN{Gregor Kajda}
\IEEEauthorblockA{\textit{Master Student, Dept. of Informatics} \\
\textit{The Faculty of Mathematics}\\
\textit{and Natural Sciences}\\
Oslo, Norway \\
grzegork@ifi.uio.no}
\and
\IEEEauthorblockN{Jonatan Hoffmann Hanssen}
\IEEEauthorblockA{\textit{Master Student, Dept. of Informatics} \\
\textit{The Faculty of Mathematics}\\
\textit{and Natural Sciences}\\
Oslo, Norway \\
jonatahh@ifi.uio.no}
}

\maketitle

\begin{abstract}

Reinforcement Learning (RL) algorithms suffer when rewards are sparse and the state-action space is large. Even tasks which seem relatively simple can prove intractable if the completion of the task requires subtasks to be completed first, or if the reward only comes when the entire task has been completed. In such cases, random exploration is unlikely to lead the agent to discover a solution to the problem. The apparent simplicity of such problems is often due to human intuition, which allow us to quickly see possible solutions to a diverse set of problems. Large Language Models (LLMs) are trained on large corpora of human written text, and have been shown to capture parts of this intuition in many tasks. In recent years, LLMs have been succesfully used to aid RL agents in more efficient exploration, and have allowed them to solve problems which previous methods have been unable to. We compare different methods for integrating an LLM into the Deep Learning RL algorithm Proximal Policy Optimization (PPO), and compare their efficiency against each other. We find that BEST METHOD gives the best sample efficiency, outperforming normal PPO by PERCENTAGE in METRIC.

\end{abstract}

\begin{IEEEkeywords}
large language models, reinforcement learning, minigrid, proximal policy optimization
\end{IEEEkeywords}

\section{Introduction}

One of the central challenges of reinforcement learning is that rewards are often both extremely rare and delayed, which means that optimizing a reinforcement learning algorithm requires significant trial and error \cite[423]{brunton}. Furthermore, in many problems the state-action spaces are enormous, meaning that uniform exploration is unlikely to find good solutions in a reasonable amount of time. Many methods have been developed to deal with this issue, for example by introducing auxilliary reward functions that use domain knowledge to reward actions which are considered good, or which reward the agent for learning novel skills. However, hand picking which actions to reward can lead to imitation rather than optimal behaviour, and novelty is not always useful \cite[1]{ellm}. In recent years, LLMs have shown remarkable capabilities in problem solving and planning \cite{sparks}, qualities which traditional RL agents often lack. Thus, a new area of research has emerged, which attemps to use the vast amount of human knowledge encoded in these models to improve the performance of reinforcement learning algorithms \cite{survey}. These LLM assisted RL agents have been able to outperform state-of-the-art RL methods in many problems \cite{omni, ellm, idm}.

These papers explore different methods of integrating LLMs into a standard reinforcement learning training loop, from altering the policy directly to introducing an auxilliary reward function. In this paper, we compare both methods in the same environment using the same deep reinforcement algorithm (PPO), and compare their results to each other to explore how LLMs best can be integrated into RL.

\section{Background and Related Work}

RL informed by natural language is a relatively new field, where research before 2018 have been limited to relatively small corpora or synthetic language \cite[1]{survey}. With the introduction of Large Language Models, many new possibilities have opened up, and in recent years there have been many successful integrations of these models into traditional RL \cite{omni, ellm, idm}.


\subsection{Large Language Models as Policy in Reinforcement Learning} 

Among the methods proposed to integrate the LLM, many include ways of making the LLM influence the policy of the RL agent. \cite{omni} considers open-ended learning algorithms in which the value of the agent attempting certain tasks can be evaluated by estimating the the interestingness of the task. The notion of interestingness is then thought of as what task would intuitively be interesting for a human to try in the context of learning in an environment. Some main factors contributing to this interestingness are how likely one is to succeed in doing the task, as well as whether learning the given task may increase the likelihood of succeeding in other tasks. Considering that LLMs are trained on extremely large amounts of human-written text containing human knowledge and intuition, this leads to the idea that if the LLM was told to propose interesting actions for the RL agent to learn in its environment, it could be able to communicate human intuition directly to the agent. This would in turn let the agent learn with increased sample efficiency as the human intuition-based input it receives would make it decide to perform intuitively smarter actions, particularly in the early stages of exploring its environment.

The algorithm proposed in this paper involves prompting the LLM with the agent's state observation, a list of tasks the agent already does well, and a list of available tasks for the LLM to deem as either interesting or boring. After sorting available tasks into interesting and boring ones, it assigns sampling weights to them, giving much higher weights to interesting tasks than boring ones, thus directly influencing the agent's policy by altering probabilities of which action it then chooses to do. In \cite{idm}, another paper proposing using the LLM as a policy optimizer, has a similar approach in which the state observation is tokenized and passed to the LLM, though in their experiment, goals and action histories are also tokenized and passed. Here, too, the LLM response is fed back into a task-specific RL model, which in turn influences policy probabilities of certain actions being taken. In general, many of the novel methods suggested involve the LLM suggesting actions for the agent to perform in a given context, and its output influencing the agent policy. 

\subsection{Large Language Models as Reward in Reinforcement Learning}

Another proposed method of involving LLMs in RL, is to have it influence the reward the agent receives for taking certain actions in certain states. In particular, \cite{ellm} cites the infrequency of rewards as a common bottleneck in RL algorithms, due to how long exploratory trajectories often have to be before the agent reaches some desirable state. This holds true especially for large, complex environments, where the probability of reaching favorable states among a huge number of non-favorable states becomes even smaller. Similarly to the papers proposing LLMs to influence policy, this paper too seeks to make use of human intuition and knowledge present in the vast training data that state-of-the-art LLMs have been trained upon, to make the agent prioritize exploring plausibly useful behaviors first, based on some intelligent forethought provided by the LLM.

However, rather than doing so through directly influencing action probabilities in the policy, the paper suggests rewarding the agent if it takes actions that are semantically similar to actions suggested by an LLM. For example, the LLM might recommend the agent to "cut down a tree". If the agent performs the "chop" action at a tree, this action can be described as "chop tree". Using a language model, we can create encodings of these sentences and compare their similarity mathematically. At certain timesteps, the LLM is prompted with a state observation from the agent, and is asked to suggest actions for the agent to take. If the agent then takes actions which are semantically similar to the suggested actions, it receives a reward. Thus, the proposed algorithm makes use of LLMs in two ways: first to generate what the LLM perceives as desirable actions, then later to assess similarity between a natural-language description of the agents action and the previously suggested actions. The first use case in particular utilizes the presence of human intuition in LLMs, as a human would often be able to intuitively figure out what is desirable to achieve in some environment.

\section{Methods}

\subsection{Problem Formulation}

We train a PPO model in a Partially Observable Markov Decision Process (POMDP). A POMDP is defined by a tuple $(S, A, T, R, O, \Omega, \gamma)$. Here, $s \in S$ denotes the state the environment is in and $a \in A$ denotes an action the agent can take. Based on the state and the action taken, the agent receives an observation $o \in \Omega$ which depends on the new environment state and the action taken by $O(o | s, a)$. $T(s' | s, a)$ denotes the state transfer function, which gives a new state based on the previous state and action. $r \in R$ is the reward given by the environment given an action and a state, while $\gamma$ is the discount factor.  The environment we use is part of Minigrid \cite{minigrid}. 

\subsection{Approach}

Using this environment, we explore three different ways of training the agent. We evaluate these models on the same Minigrid environments and compare their performance against each other.\\

\subsubsection{Standard Proximal Policy Optimization}

A standard PPO algorithm with no LLM integration, as a baseline. In this case, we use StableBaselines' implementation with the following hyperparameters: HYPERPARAMETERS HERE.

\subsubsection{PPO with LLM Integrated in the Policy Function}

In this case, the LLM is integrated as part of the PPO algorithm, directly influencing the actions taken. This is done as follows: First, the observation is captioned and given as input to the LLM, which is prompted to suggest an action. Then, all actions that the agent could take given the current state are described in natural language. All possible actions are compared with the suggested action, and given a value which denotes their semantic similarity. These values are joined with the probability values that the normal PPO algorithm outputs and normalized, and a new probability distribution is created, where semantically similar actions are weighted higher than others. We call this the Actor-Advisor-Critic Model.

\subsubsection{PPO with LLM Integrated in the Reward Function}

The agent is trained as in the baseline. The agent's state observation is encoded to natural text, and sent as part of a prompt to the LLM. This may be done at every timestep, or after some interval of timesteps. In addition to the state observation, the LLM is also asked to respond with a list of recommendations. These recommendations can be proposed actions for the agent to perform, or goal states that the agent should reach. The agent then acts independently of the LLM for one or more timesteps, as dictated by the baseline RL algorithm it is trained with. As it does so, its actions are tracked and encoded to natural text. Alternatively, a new state observation is generated after the timesteps, and also encoded as natural text. Then, the LLM is used as a tool for semantic similarity comparison, comparing either its own formerly proposed actions to the actions performed by the agent, or comparing its proposed goal state to the new state reached by the agent. Finally, high semantic similarity gives an additional reward which is implemented in the reward function of the agent. 

\subsection{Large Language Model used}

The LLM that is used is Meta's Llama2 \cite{llama}, which is an open source language model. We use the smallest model size, which has 7 billion parameters.

\section{Experiments}

We perform experiments to compare different methods of integrating LLMs with the PPO algorithm, specifically comparing their ability to guide the early exploration of an RL model in a reward-sparse environment.

\subsection{Minigrid Environments}

For our experiments, we choose the Minigrid library. The Minigrid library contains simple grid world environments. An important characteristic of all these environments is that they only give reward on the completion of the goal of the environment. In other words, the environment only ever gives out a single reward per episode, at the end of the episode. Goals in minigrid are often simple tasks, such as "unlock the door", "go to the goal", "pick up the red chest". Completing these goals often requires the completion of subtasks; going to the goal might require opening a door, and opening the door might require picking up a key. These environments are partially observable, which means that the agent only receives a seven by seven observation grid, not the entire grid. Figure \ref{doorkeyenv}, shows a typical minigrid environment. In this case, the agent (the red arrow) only sees the left room, and does not know where the goal is.

\begin{figure}[h]
\centerline{\includegraphics[width=3.25in]{figure/doorkeyenv.png}}
\caption{The Minigrid DoorKey environment}
\label{doorkeyenv}
\end{figure}

A more complicated environment can be seen in figure \ref{blockedunlockpickupenv}. Completing the goal requires picking up the ball and dropping it somewhere else, picking up the key, using it to unlock the door, dropping the key (the agent can only carry one item at once) and finally picking up the box. With no intermediate rewards, this is a very difficult task for traditional RL methods. The exact configuration of Minigrid environments is random; the position of the agent, the placement of the walls and items within the grid all vary from episode to episode.

\begin{figure}[h]
\centerline{\includegraphics[width=3.25in]{figure/blockedunlockpickupenv.png}}
\caption{The Minigrid BlockedUnlockPickup environment}
\label{blockedunlockpickupenv}
\end{figure}

Some environments can only be effectively solved by relying on language (for example, environments where the goal text describes a specific object to be picked up, which changes every time), while others can be solved without (for example, if the goal always is to pick up a key). We only use environments that can be solved without language, so that the baseline PPO algorithm is not at an unfair disadvantage.

\subsection{Our chosen environments}

We select NUMBER different Minigrid environments; DoorKey, OTHERENV, OTHERENV. Example instances of these environments can be found in the appendix. For each environment we test our three different models; PPO with no LLM integration, PPO with LLM reward shaping and PPO with LLM policy integration. All methods use the same hyperparameters for the PPO, which can be found the appendix. For each of these methods, we run PPO for 2000 episodes of 1024 steps, and track the reward\footnote{The true environment reward, not including pseudo-rewards given by the LLM} received for each one. We do this eight times per method, and compare the average results against eachother.

\section{Results}

\subsection{DoorKey environment}

As we can see from figure \ref{doorkeyresults}, LLM integration gives a noticable improvement to the performance of the model, not just in the early stages of training, but throughout the entire training process. Averaging over all episodes of all eight runs per model, we find that LLM reward shaping receives $24\%$ more reward per episode than PPO trained without LLM rewards. Furthermore, this is an improvement that comes at very little computational cost, because LLM responses can be cached between runs. Of the eight runs on this environment using LLM reward shaping, the last three runs never queried the LLM at all, only relying on the cache of previous answers. In all methods, we see that the variance is very large ($0.28$ and $0.31$ for LLM reward shaping and baseline, respectively). Given that there is no reward given for a partially completed task, this is not unexpected.

\begin{figure}[h]
\centerline{\includegraphics[width=3.25in]{figure/doorkeyresults.png}}
\caption{Reward per episode for the three (two) different methods in the DoorKey environment}
\label{doorkeyresults}
\end{figure}


\section{Conclusion}

\section*{Ethics Statement}

In accordance with the NeurIPS Code of Ethics \cite{ethics}, we outline ethical considerations related to this project, covering potential consequences from the research, and how potential risks may be mitigated. Considering that our research does not involve participants nor sensitive data, concerns related to how such factors are treated will not be discussed.

\subsection{Societal Impact and Potential Harmful Consequences}

Due to how this research project revolves around the integration of LLMs in the RL domain, many of the same concerns that exist in the domain of LLMs themselves are extended to the RL domain. A major concern is that of safety; if the RL agent is to act in an environment where failure to act optimally could be critical, it is paramount that the agent does not enter bad states due to the existing randomness in the RL framework. The LLM in itself predicts stochastically, and thus may add even more noise to the framework. In the worst case, it could amplify probabilities of the agent choosing dangerous actions due to this added noise.

Another topic of concern is that of bias and fairness; the premise for why an LLM might help an RL agent in a large environment is because of its inherent human knowledge, stemming from the human-written texts it has been trained upon. However, human-written texts are an obvious potential source to biased opinions, and the LLM may have learned such biased and/or unfair opinions. Then, in an example where the RL agent is deployed in environments where an ethical choice has to be made, this choice could be influenced by whatever bias may be present in what the LLM suggests to the agent.

\subsection{Impact Mitigation Measures}

As is exemplified above, major ethical concerns regarding the use of our proposed algorithms arise when they are deployed for critical tasks, for instance tasks in real-life dangerous environments, or environments where ethical considerations have to be addressed. Therefore, an agent trained on our proposed RL framework for such critical tasks should be carefully tested before deployment to see if, and how often, critical deviations occur.

In the further development of this and similar algorithms, we also consider it essential to put effort into trying to understand the causality behind certain actions being taken, as is being done in the field of explainable artificial intelligence (XAI). When involving one major ML framework like that of LLMs into RL, we would ideally want to see exactly where and how much the LLM contributes to the RL networks being updated. This would, in turn, help in the work to mitigate whatever biases and variance that may be added to the RL framework from integrating the LLM.

\section*{Acknowledgment}

I would like to acknowledge myself for being a absolute legend.

\bibliographystyle{apalike}
\bibliography{bibliography}


\section*{Appendix}

% \begin{table*}[h]
% \caption{PPO hyperparameters}
% \begin{center}
% \label{hyperparams}
% \begin{tabular}{c | l l l l l l l l} 
% % & & \\ % put some space after the caption
% % \hline
% Parameter & Gamma & Steps/episode & Epochs/episode & Batches/epoch & Clip & Value loss coeff. & Entropy coeff. & Learning rate \\
% \hline
% Value & 0.99 & 1024 & 10 & 8 & 0.2 & 0.3 & 0 & 3.0e-4 \\
% \hline
% \end{tabular}
% \end{center}
% \end{table*}
\begin{table*}[h]
\caption{PPO hyperparameters}
\begin{center}
\label{hyperparams_transposed}
\begin{tabular}{c | c}
Parameter & Value \\
\hline
Gamma & 0.99 \\
Steps/episode & 1024 \\
Epochs/episode & 10 \\
Batches/epoch & 8 \\
Clip & 0.2 \\
Value loss coeff. & 0.3 \\
Entropy coeff. & 0 \\
Learning rate & 3.0e-4 \\
\end{tabular}
\end{center}
\end{table*}



\end{document}
